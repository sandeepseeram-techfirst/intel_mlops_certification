### Compute Aware AI Systems 

##### CPU
 commonly used for data processing, model development, and scalable inference. Handle diverse tasks throughout the ML pipeline. 

##### GPU
 generally used in AI workloads to accelerate compute-intensive tasks, leveraging their parallel architeccture and optimized floating point operations for faster and more efficient computations. Using GPU's can enhance the training and inference processes of AI models. 

##### Clusters 
distributed computational resources and scalability to handle large scale data processing, model training, and inference tasks. 

##### FPGA's 
accelerate inference tasks by offloading computations from CPU's and GPU's. 
 - Helps in improved performance, reduced latency, and lower power consumption in AI applications. 

##### EDGE 
Edge compute, is to bring computational power and AI capabilities closer to the data source or end-user, enabling real-time processing, reduced latency and privacy sensitive applications. 


### Heterogeneous AI Systems 
Combination of CPU's, GPU's, VPU's, TPU's & Accelerators 