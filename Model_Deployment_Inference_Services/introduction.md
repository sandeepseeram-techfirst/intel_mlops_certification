#### Model Deployment and Inference Services 

- Compute Infrastructure and Resilience Measures in Scalable Inference Services. 


#### Hot in memory  
- Models can be deployed by loading and compiling them into the ML prediction service's memory. 

![alt text](hotmodel.png)


#### ML Prediction Service

- The ML prediction service receives inputs and delivers suitable responses either directly or via post-processing. 
- Its a modular part of a broader production inference pipeline. 

![alt text](mlpredictionservice.png)


#### Inference Modes 

- Real-time or Online Inference 
- Batch Inference 

