### Compute Aware AI Systems 

##### CPU
 commonly used for data processing, model development, and scalable inference. Handle diverse tasks throughout the ML pipeline. 

##### GPU
 generally used in AI workloads to accelerate compute-intensive tasks, leveraging their parallel architeccture and optimized floating point operations for faster and more efficient computations. Using GPU's can enhance the training and inference processes of AI models. 

##### Clusters 
distributed computational resources and scalability to handle large scale data processing, model training, and inference tasks. 

##### FPGA's 
accelerate inference tasks by offloading computations from CPU's and GPU's. 
 - Helps in improved performance, reduced latency, and lower power consumption in AI applications. 
 
- EDGE 