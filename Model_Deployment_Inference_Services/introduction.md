#### Model Deployment and Inference Services 

- Compute Infrastructure and Resilience Measures in Scalable Inference Services. 


#### Hot in memory 
- Models can be deployed by loading and compiling them into the ML prediction service's memory. 

![alt text](hotmodel.png)